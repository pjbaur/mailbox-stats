This is day 4 of this project.

---

# Day 4 Goal

**Fetch enough data to aggregate:**

* Message count per sender
* Total message size per sender
* Optional: attachment indicators
* Do it **fast**, **cheap**, and **correctly**

---

## 1Ô∏è‚É£ Decide the Minimal Data Contract (This Is Key)

You do **not** want full messages. You want *just enough* to answer questions.

### Use:

```python
format="metadata"
```

### From each message, extract:

* `payload.headers`:

  * `From`
  * `Date` (optional if you want time windows later)
* `sizeEstimate` ‚úÖ (this is gold)
* `payload.parts` (structure only, not bodies)

### Explicitly skip:

* Message body
* Attachment binary data
* Snippets (optional, not useful here)

üìå **Important insight**
`sizeEstimate` already includes attachments. That means:

* You can rank senders by ‚Äúhow much mailbox space they consume‚Äù
* You don‚Äôt need to download attachments to count their impact

---

## 2Ô∏è‚É£ Attachment Detection Without Fetching Attachments

You can infer attachments safely and cheaply:

```python
def has_attachment(payload):
    if not payload or "parts" not in payload:
        return False

    for part in payload["parts"]:
        if part.get("filename"):
            return True
        if part.get("body", {}).get("attachmentId"):
            return True
    return False
```

Store:

* `has_attachment: bool`
* Optional: `attachment_count` if you want later

This costs **zero extra API calls**.

---

## 3Ô∏è‚É£ Normalize the Sender (Critical for Aggregation)

Raw `From` headers are chaos:

```
"Amazon.com <order-update@amazon.com>"
"AMAZON <no-reply@amazon.com>"
```

### Normalize to:

* Email domain (recommended): `amazon.com`
* Or full email address

Example:

```python
def normalize_sender(from_header):
    # extract email, lowercase, strip display name
```

üìå **Recommendation**
Use **domain-level aggregation first**, keep raw sender for drill-down later.

---

## 4Ô∏è‚É£ Aggregation Data Model (In-Memory First)

Use a dict keyed by sender:

```python
stats = {
  "amazon.com": {
      "message_count": 1234,
      "total_size": 987654321,
      "messages_with_attachments": 456
  }
}
```

Update per message:

* `+1` message_count
* `+sizeEstimate`
* `+1` attachments if detected

This lets you:

* Stream results
* Avoid storing per-message rows unless you want historical detail later

---

## 5Ô∏è‚É£ Pagination Strategy (Quota-Safe)

* `users.messages.list` gives IDs only
* Page size: `maxResults=500`
* Fetch messages in batches
* Log progress every N messages

Example checkpoint log:

```
Fetched 5,000 / ~77,000 messages (6.4%)
Elapsed: 00:02:11
```

This gives you:

* Confidence
* Kill-switch capability
* Debug visibility

---

## 6Ô∏è‚É£ Output Artifacts to Produce Today

You should end Day 4 with **actual answers**, not just plumbing.

### Minimum outputs:

* Top 20 senders by message count
* Top 20 senders by total size (MB/GB)
* Percentage of mailbox size from top N senders

Optional but powerful:

* CSV export
* JSON snapshot for later dashboards

---

## 7Ô∏è‚É£ What You Explicitly Do *Not* Do Today

‚ùå Full message fetch
‚ùå Body decoding
‚ùå Thread reconstruction
‚ùå Attachment downloads
‚ùå Database persistence (unless you already planned it)

Those are Day 5+ problems.

---

## Day 4 ‚ÄúShipped‚Äù Definition ‚úÖ

You can confidently say:

> ‚ÄúI know who fills my mailbox, how much space they consume, and whether attachments are the culprit.‚Äù

* Add timing + quota guards
* Persist results to SQLite
